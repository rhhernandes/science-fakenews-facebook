{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "laughing-exploration",
   "metadata": {},
   "source": [
    "# Scraping Media Bias websites list and their Facebook pages\n",
    "\n",
    "This script will go through the lists of websites tagged as \"least biased\", \"conspiracy-pseudoscience\" and \"pro-science\" on [Media Bias/Fact Check](https://mediabiasfactcheck.com/):\n",
    "\n",
    "* https://mediabiasfactcheck.com/center/\n",
    "* https://mediabiasfactcheck.com/conspiracy/\n",
    "* https://mediabiasfactcheck.com/pro-science/\n",
    "\n",
    "Later, it will go through each of these sites to gather their Facebook pages URLs.\n",
    "\n",
    "These Facebook pages will later be listed in a format compatible with [CrowdTangle](https://www.crowdtangle.com/)'s import function. The tool will later be used for analysing engagement for these pages.\n",
    "\n",
    "CrowdTangle requests data in a .csv file, using the `Page or Account URL,List` template, one page per line. List being the name of the list the referred page will be a part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from IPython.display import clear_output\n",
    "import timeit\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-legislature",
   "metadata": {},
   "source": [
    "## Generating the full websites lists\n",
    "\n",
    "Gather information from Media Bias/Fact check.\n",
    "\n",
    "These pages already contain the URL for almost all of the websites under each category in a table.\n",
    "\n",
    "The scraped content will be added to a list of dictionaries (`contents`) one for each entry. In the end, each will contain:\n",
    "\n",
    "```\n",
    "{\n",
    "    'category' : 'least-biased|conspiracy-pseudoscience|pro-science', \n",
    "    'website_name' : 'name',\n",
    "    'url' : 'website's url',\n",
    "    'report' : 'media bias report url',\n",
    "    'facebook_pages' : 'the list of all facebook.com links found in the websites homepages',\n",
    "    'number_facebook_urls' : int # no. of Facebook URLs found on the page\n",
    "}\n",
    "```\n",
    "\n",
    "Each of these entries will need to be inspected one by one later. This is necessary because the script will gather all the Facebook pages in each of the entries' homepage, in case there are multiple ones. \n",
    "\n",
    "The data will be saved to a .xlsx file for making life easier when updating information during this inspection, `./data/interim/extracted_data.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "announced-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {'least-biased' : 'https://mediabiasfactcheck.com/center/',\n",
    "       'conspiracy-pseudoscience' : 'https://mediabiasfactcheck.com/conspiracy/',\n",
    "       'pro-science' : 'https://mediabiasfactcheck.com/pro-science/'}\n",
    "\n",
    "contents = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-quilt",
   "metadata": {},
   "source": [
    "Each line with the URLs follow one of two formats: either it contains just the URL, or it contains the website name and the URL in parentheses. They will be extracted using the `extract_url` function below.\n",
    "\n",
    "A second part of the script will go through the ones that do not have the URLs in the lists already and gather the information from their reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tribal-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url(text):\n",
    "    '''\n",
    "    Extracts URL information from each row's website name.\n",
    "    \n",
    "    Args:\n",
    "    text: STR - the text information for each row (row.text)\n",
    "    \n",
    "    Output:\n",
    "    STR\n",
    "    '''\n",
    "    if '(' in text:\n",
    "        # THE URL IS ALWAYS THE LAST PART WHEN IN PARENTHESES\n",
    "        url = text.split('(')[-1].strip(')')\n",
    "    elif '.' in text:\n",
    "        url = text\n",
    "    else:\n",
    "        return '-'\n",
    "    if url.startswith('http'):\n",
    "        return url\n",
    "    else:\n",
    "        return ('https://' + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-conservation",
   "metadata": {},
   "source": [
    "### Scraping loops\n",
    "\n",
    "*Note: using nested for loops is not the most performatic way of handling this task, but it should do the trick just fine given that it is not really that much data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stuck-damage",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON CATEGORY # 1 / 3 - least-biased\n",
      "WORKING ON CATEGORY # 2 / 3 - conspiracy-pseudoscience\n",
      "WORKING ON CATEGORY # 3 / 3 - pro-science\n",
      "\n",
      "COMPLETE\n",
      "********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_ran = 0\n",
    "\n",
    "for category, url in urls.items():\n",
    "    categories_ran += 1\n",
    "    print('WORKING ON CATEGORY #', categories_ran, '/', len(urls), '-',  category)\n",
    "    \n",
    "    # COLLECT THE PAGE\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # GETS THE TABLE WITH THE WEBSITE LIST\n",
    "    table = soup.find( 'table', {'id':'mbfc-table'} )\n",
    "    \n",
    "    for row in table.findAll(\"tr\"):\n",
    "        # SKIPS ROWS WITHOUT ANCHOR <a> TAG, FOR THEY ARE EMPTY LINES\n",
    "        if not row.find('a'):\n",
    "            continue\n",
    "        else:\n",
    "            data = {'category' : category,\n",
    "                    'website_name' : row.text,\n",
    "                    'url' : extract_url(row.text),\n",
    "                    'report' : 'https://mediabiasfactcheck.com' + row.find('a')['href']}\n",
    "            contents.append(data)\n",
    "    \n",
    "print('')\n",
    "print('COMPLETE')\n",
    "print('*' * len('COMPLETE'))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-digest",
   "metadata": {},
   "source": [
    "#### Pages lacking URL\n",
    "\n",
    "Goes through the report pages for the lines which did not contain the website URL in order to extract this info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "possible-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_report(report_url):\n",
    "    '''\n",
    "    Extracts URL information from a Media Bias/Fact Check report page.\n",
    "    \n",
    "    Args:\n",
    "    url: STR - the web address for the report page\n",
    "    \n",
    "    Output:\n",
    "    STR - the URL for the website in the report\n",
    "    '''    \n",
    "    report_page = requests.get(report_url)\n",
    "    report_soup = BeautifulSoup(report_page.text, 'html.parser')\n",
    "    report_content = report_soup.find('div', {'class':'entry-content'} )\n",
    "    for p in report_content.find_all('p'):\n",
    "        if p.text.startswith('Source: '):\n",
    "            return p.find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marked-certification",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON https://mediabiasfactcheck.com/adweek/\n",
      "WORKING ON https://mediabiasfactcheck.com/air-force-times/\n",
      "WORKING ON https://mediabiasfactcheck.com/allafrica/\n",
      "WORKING ON https://mediabiasfactcheck.com/aptn-news/\n",
      "WORKING ON https://mediabiasfactcheck.com/army-times/\n",
      "WORKING ON https://mediabiasfactcheck.com/biloxi-sun-herald/\n",
      "WORKING ON https://mediabiasfactcheck.com/bozeman-daily-chronicle/\n",
      "WORKING ON https://mediabiasfactcheck.com/burnett-county-sentinel/\n",
      "WORKING ON https://mediabiasfactcheck.com/denton-record-chronicle/\n",
      "WORKING ON https://mediabiasfactcheck.com/eagle-tribune/\n",
      "WORKING ON https://mediabiasfactcheck.com/elko-daily-free-press/\n",
      "WORKING ON https://mediabiasfactcheck.com/hastings-tribune/\n",
      "WORKING ON https://mediabiasfactcheck.com/how-to-geek/\n",
      "WORKING ON https://mediabiasfactcheck.com/longview-news-journal/\n",
      "WORKING ON https://mediabiasfactcheck.com/norfolk-daily-news/\n",
      "WORKING ON https://mediabiasfactcheck.com/south-bend-tribune/\n",
      "WORKING ON https://mediabiasfactcheck.com/statesville-record-landmark/\n",
      "WORKING ON https://mediabiasfactcheck.com/statista/\n",
      "WORKING ON https://mediabiasfactcheck.com/telegram-gazette/\n",
      "WORKING ON https://mediabiasfactcheck.com/battlefords-news-optimist/\n",
      "WORKING ON https://mediabiasfactcheck.com/the-herald-dispatch/\n",
      "WORKING ON https://mediabiasfactcheck.com/the-register-uk/\n",
      "WORKING ON https://mediabiasfactcheck.com/the-star-democrat/\n",
      "WORKING ON https://mediabiasfactcheck.com/traverse-city-record-eagle/\n",
      "WORKING ON https://mediabiasfactcheck.com/winona-daily-news/\n",
      "WORKING ON https://mediabiasfactcheck.com/winston-salem-journal/\n",
      "WORKING ON https://mediabiasfactcheck.com/wral/\n",
      "WORKING ON https://mediabiasfactcheck.com/alt-market/\n",
      "WORKING ON https://mediabiasfactcheck.com/autism-investigated/\n",
      "WORKING ON https://mediabiasfactcheck.com/ecology-news/\n",
      "WORKING ON https://mediabiasfactcheck.com/global-skywatch/\n",
      "WORKING ON https://mediabiasfactcheck.com/neuronation/\n",
      "WORKING ON https://mediabiasfactcheck.com/occupy-yourself/\n",
      "WORKING ON https://mediabiasfactcheck.com/offguardian/\n",
      "WORKING ON https://mediabiasfactcheck.com/south-front/\n",
      "WORKING ON https://mediabiasfactcheck.com/alliance-for-science/\n",
      "WORKING ON https://mediabiasfactcheck.com/health-affairs-journal/\n",
      "WORKING ON https://mediabiasfactcheck.com/precision-vaccinations/\n",
      "WORKING ON https://mediabiasfactcheck.com/sage-journals/\n",
      "WORKING ON https://mediabiasfactcheck.com/sciencedirect/\n",
      "WORKING ON https://mediabiasfactcheck.com/significance-magazine/\n",
      "WORKING ON https://mediabiasfactcheck.com/skeptical-inquirer/\n",
      "WORKING ON https://mediabiasfactcheck.com/vaxopedia/\n",
      "\n",
      "COMPLETE\n",
      "********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entry in contents:\n",
    "    if entry['url'] == '-':\n",
    "        print('WORKING ON', entry['report'])\n",
    "        entry['url'] = extract_url_report(entry['report'])\n",
    "        time.sleep(1)\n",
    "        \n",
    "print('')\n",
    "print('COMPLETE')\n",
    "print('*' * len('COMPLETE'))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-frederick",
   "metadata": {},
   "source": [
    "## Enters each webpage and extracts any Facebook URL\n",
    "\n",
    "First, try to find all the Facebook links on a page using `requests`/`BeautifulSoup`. All the unique values will be put on a list. The funcion will return both the number of unique Facebook URLs found and the complete list. Having the number of URLs found should make it easier when going through the records one by one to check the Facebook links.\n",
    "\n",
    "Some of the pages in the analysis are dynamic, so we will use `Selenium` to rescan the ones that failed to find any Facebook URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "driven-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_requests(extract_url):\n",
    "    '''\n",
    "    Uses requests for extracting page data.\n",
    "    \n",
    "    Args:\n",
    "    extract_url - STR - url for the extraction\n",
    "    \n",
    "    Returns:\n",
    "    BeutifulSoup object with the parsed page.\n",
    "    '''\n",
    "    page = requests.get(extract_url, timeout=60)\n",
    "    return BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "def generate_selenium(extract_url):\n",
    "    '''\n",
    "    Sets the Selenium parameters for extracting the page data.\n",
    "    \n",
    "    Args:\n",
    "    extract_url - STR - url for the extraction\n",
    "    \n",
    "    Returns:\n",
    "    BeutifulSoup object with the parsed page.\n",
    "    '''\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    driver.set_page_load_timeout(60)\n",
    "    driver.get(extract_url)\n",
    "    \n",
    "    extract_soup = BeautifulSoup(driver.page_source)    \n",
    "    driver.quit()\n",
    "\n",
    "    return extract_soup\n",
    "    \n",
    "def extract_facebook(page_object):\n",
    "    '''\n",
    "    Uses BeautifulSoup to extract all links pointing to Facebook from a requests or Selenium source code object.\n",
    "    \n",
    "    Args:\n",
    "    page_object: requests or Selenium source code object\n",
    "    \n",
    "    Output:\n",
    "    List of Facebook pages.\n",
    "    '''\n",
    "    facebook_urls = list()\n",
    "    \n",
    "    for a in page_object.find_all('a'):\n",
    "        if a.has_attr('href') and 'facebook.com' in a['href']:\n",
    "            if a['href'] not in facebook_urls:\n",
    "                facebook_urls.append(a['href'])\n",
    "    \n",
    "    if len(facebook_urls) > 0:\n",
    "        return (facebook_urls, len(facebook_urls))\n",
    "    else:\n",
    "        return (['None found'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thousand-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON URL # 949 out of 949 - https://www.zmescience.com\n",
      "Progress:  100.0 %\n",
      "Current run time: 28.82 minutes\n",
      "Expected run time: 28.82 minutes\n"
     ]
    }
   ],
   "source": [
    "total_runs = 0\n",
    "start = timeit.default_timer()\n",
    "content_error = list()\n",
    "\n",
    "for entry in contents:\n",
    "    clear_output(wait=True)\n",
    "    total_runs += 1\n",
    "    \n",
    "    try:\n",
    "        if entry['url'] != None and entry['url'].startswith('http'):\n",
    "            soup = generate_requests(entry['url'])\n",
    "            extracted_data = extract_facebook(soup)\n",
    "            entry['facebook_page'] = extracted_data[0]\n",
    "            entry['number_facebook_urls'] = extracted_data[1]\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        expected_time = np.round((stop-start) / (total_runs / len(contents)) / 60, 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        content_error.append((entry, e))\n",
    "        continue\n",
    "\n",
    "    print('WORKING ON URL #', total_runs, 'out of', len(contents), '-', entry['url'])\n",
    "    print('Progress: ', np.round((total_runs/len(contents) * 100), 2), '%')\n",
    "    print('Current run time:', np.round((stop - start)/60, 2), 'minutes')\n",
    "    print('Expected run time:', expected_time, 'minutes') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-appeal",
   "metadata": {},
   "source": [
    "### Checks errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stable-continuity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "christian-reception",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'category': 'least-biased',\n",
       "   'website_name': 'Asia Times (www.atimes.com)',\n",
       "   'url': 'https://www.atimes.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/asia-times/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.atimes.com\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.atimes.com\\' doesn\\'t match either of \\'*.parkingcrew.net\\', \\'parkingcrew.net\\'\")))'))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Belleville News-Democrat (www.bnd.com)',\n",
       "   'url': 'https://www.bnd.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/belleville-news-democrat/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='www.bnd.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Biloxi Sun Herald',\n",
       "   'url': 'http://sunherald.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/biloxi-sun-herald/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='www.sunherald.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Daily Journal (Illinois)',\n",
       "   'url': 'https://Illinois',\n",
       "   'report': 'https://mediabiasfactcheck.com/daily-journal-illinois/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='illinois', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F6495CE48>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Del Rio News-Herald (delrionewsherald.com)',\n",
       "   'url': 'https://delrionewsherald.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/del-rio-news-herald-bias/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='delrionewsherald.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F63CB0A88>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'DifferenceBetween.net (www.differencebetween.net)',\n",
       "   'url': 'https://www.differencebetween.net',\n",
       "   'report': 'https://mediabiasfactcheck.com/differencebetween-net/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.differencebetween.net\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.differencebetween.net\\' doesn\\'t match \\'*.opalstack.com\\'\")))'))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Digital Journal (www.digitaljournal.com)',\n",
       "   'url': 'https://www.digitaljournal.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/digital-journal/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='www.digitaljournal.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F638C41C8>: Failed to establish a new connection: [WinError 10060] Uma tentativa de conexão falhou porque o componente conectado não respondeu\\\\r\\\\ncorretamente após um período de tempo ou a conexão estabelecida falhou\\\\r\\\\nporque o host conectado não respondeu'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Homeland Security News Wire (www.homelandsecuritynewswire.com)',\n",
       "   'url': 'https://www.homelandsecuritynewswire.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/homeland-security-news-wire/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='www.homelandsecuritynewswire.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F638F4C48>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Idaho Statesman (www.idahostatesman.com)',\n",
       "   'url': 'https://www.idahostatesman.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/idaho-statesman/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='www.idahostatesman.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'International Press Institute (www.freemedia.at)',\n",
       "   'url': 'https://www.freemedia.at',\n",
       "   'report': 'https://mediabiasfactcheck.com/international-press-institute/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='www.freemedia.at', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F64041208>: Failed to establish a new connection: [WinError 10060] Uma tentativa de conexão falhou porque o componente conectado não respondeu\\\\r\\\\ncorretamente após um período de tempo ou a conexão estabelecida falhou\\\\r\\\\nporque o host conectado não respondeu'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Justice Denied (justicedenied.org)',\n",
       "   'url': 'https://justicedenied.org',\n",
       "   'report': 'https://mediabiasfactcheck.com/justice-denied/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'justicedenied.org\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'justicedenied.org\\' doesn\\'t match either of \\'*.bizland.com\\', \\'bizland.com\\'\")))'))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'KAIT8-TV (kait8.com)',\n",
       "   'url': 'https://kait8.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/kait8-tv/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='kait8.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'KBMT – 12NewsNow.com',\n",
       "   'url': 'https://KBMT – 12NewsNow.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/kbmt-12newsnow/'},\n",
       "  requests.exceptions.InvalidURL('Failed to parse: https://KBMT – 12NewsNow.com')),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'KWTV-News9 (news9.com)',\n",
       "   'url': 'https://news9.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/kwtv-news9/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.ProtocolError('Connection aborted.',\n",
       "                                                                       OSError(0,\n",
       "                                                                               'Error')))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'MLive (Michigan – Booth Newpapers)',\n",
       "   'url': 'https://Michigan – Booth Newpapers',\n",
       "   'report': 'https://mediabiasfactcheck.com/mlive-michigan-booth-newspapers/'},\n",
       "  requests.exceptions.InvalidURL('Failed to parse: https://Michigan – Booth Newpapers')),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Navy Times (navytimes.com)',\n",
       "   'url': 'https://navytimes.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/navy-times/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='navytimes.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F63805608>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Russian Media Monitor (www.russialies.com)',\n",
       "   'url': 'https://www.russialies.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/russian-media-monitor/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='www.russialies.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)')))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Shareably (www.shareably.net)',\n",
       "   'url': 'https://www.shareably.net',\n",
       "   'report': 'https://mediabiasfactcheck.com/shareably/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.shareably.net\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.shareably.net\\' doesn\\'t match either of \\'shareably.net\\', \\'animalchannel.co\\', \\'homehacks.co\\', \\'parentingisnteasy.co\\', \\'relieved.co\\', \\'seeitlive.co\\', \\'shareably.co\\', \\'spotlightstories.co\\', \\'sweetandsavory.co\\'\")))'))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'The Herald Journal (Utah)',\n",
       "   'url': 'https://Utah',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-herald-journal-utah/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='utah', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F6316AE88>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'The Herald News (Massachusetts)',\n",
       "   'url': 'https://Massachusetts',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-herald-news-massachusetts/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='massachusetts', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F638A56C8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'The Herald-Sun (Durham, North Carolina) (www.heraldsun.com)',\n",
       "   'url': 'https://www.heraldsun.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-herald-sun-durham-north-carolina/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='www.heraldsun.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'The Macon Telegraph (www.macon.com)',\n",
       "   'url': 'https://www.macon.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-macon-telegraph/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='www.macon.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'The Modesto Bee (www.modbee.com)',\n",
       "   'url': 'https://www.modbee.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-modesto-bee/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='www.modbee.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'The Monitor (Texas)',\n",
       "   'url': 'https://Texas',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-monitor-texas/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='texas', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F62E97808>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'The Republic (Indiana) (therepublic.com)',\n",
       "   'url': 'https://therepublic.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-republic-indiana/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'therepublic.com\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'therepublic.com\\' doesn\\'t match either of \\'cloudfront.net\\', \\'*.cloudfront.net\\'\")))'))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Transpartisan Review (transpartisanreview.com/)',\n",
       "   'url': 'https://transpartisanreview.com/',\n",
       "   'report': 'https://mediabiasfactcheck.com/transpartisan-review/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='transpartisanreview.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1076)')))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Tri-City Herald (tri-cityherald.com)',\n",
       "   'url': 'https://tri-cityherald.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/tri-city-herald/'},\n",
       "  requests.exceptions.ReadTimeout(urllib3.exceptions.ReadTimeoutError(\"HTTPSConnectionPool(host='www.tri-cityherald.com', port=443): Read timed out. (read timeout=60)\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'Washington Journal (C-Span)',\n",
       "   'url': 'https://C-Span',\n",
       "   'report': 'https://mediabiasfactcheck.com/washington-journal/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='c-span', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F632B6D48>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'WETM – MyTwinTiers.com',\n",
       "   'url': 'https://WETM – MyTwinTiers.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/wetm/'},\n",
       "  requests.exceptions.InvalidURL('Failed to parse: https://WETM – MyTwinTiers.com')),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'WTVF – Newschannel5 (newschannel5.com)',\n",
       "   'url': 'https://newschannel5.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/wtvf-newschannel5/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='newschannel5.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F64513A48>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\"))),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'WVEC – 13newsnow.com',\n",
       "   'url': 'https://WVEC – 13newsnow.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/wvec-13newsnow/'},\n",
       "  requests.exceptions.InvalidURL('Failed to parse: https://WVEC – 13newsnow.com')),\n",
       " ({'category': 'least-biased',\n",
       "   'website_name': 'WXIA – 11alive.com',\n",
       "   'url': 'https://WXIA – 11alive.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/wxia-11alive-com/'},\n",
       "  requests.exceptions.InvalidURL('Failed to parse: https://WXIA – 11alive.com')),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Above Top Secret (www.abovetopsecret.com)',\n",
       "   'url': 'https://www.abovetopsecret.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/above-top-secret/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='www.abovetopsecret.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F641159C8>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\"))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Autism Investigated',\n",
       "   'url': 'https://www.autisminvestigated.com/',\n",
       "   'report': 'https://mediabiasfactcheck.com/autism-investigated/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.ProtocolError('Connection aborted.',\n",
       "                                                                       OSError(0,\n",
       "                                                                               'Error')))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Earth We are One (EWAO) (ewao.com)',\n",
       "   'url': 'https://ewao.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/earth-one-ewao/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='ewao.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F63E9F6C8>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))\"))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Firefighters for 9/11 Unity and Truth (www.ff911truthandunity.org)',\n",
       "   'url': 'https://www.ff911truthandunity.org',\n",
       "   'report': 'https://mediabiasfactcheck.com/firefighters-for-9-11-truth-and-unity/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='www.ff911truthandunity.org', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1076)')))\"))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'From the Trenches World Report (www.fromthetrenchesworldreport.com)',\n",
       "   'url': 'https://www.fromthetrenchesworldreport.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/from-the-trenches-world-report/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.fromthetrenchesworldreport.com\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.fromthetrenchesworldreport.com\\' doesn\\'t match \\'fromthetrenchesworldreport.com\\'\")))'))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Homeopathy Journal (www.homeopathyjournal.net)',\n",
       "   'url': 'https://www.homeopathyjournal.net',\n",
       "   'report': 'https://mediabiasfactcheck.com/homeopathy-journal/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='www.homeopathyjournal.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F63D2C288>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\"))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Institute for Creation Research (icr.org)',\n",
       "   'url': 'https://icr.org',\n",
       "   'report': 'https://mediabiasfactcheck.com/institute-for-creation-research-icr/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'icr.org\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'icr.org\\' doesn\\'t match \\'www.icr.org\\'\")))'))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'International Climate Science Coalition (ICSC)',\n",
       "   'url': 'https://ICSC',\n",
       "   'report': 'https://mediabiasfactcheck.com/international-climate-science-coalition/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='icsc', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F639A1248>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Medical Medium (medicalmedium.com)',\n",
       "   'url': 'https://medicalmedium.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/medical-medium/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'medicalmedium.com\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'medicalmedium.com\\' doesn\\'t match either of \\'worldsecuresystems.com\\', \\'*.businesscatalyst.com\\', \\'*.worldsecuresystems.com\\', \\'businesscatalyst.com\\'\")))'))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Natural Awakenings Magazine (www.naturalawakeningsmag.com)',\n",
       "   'url': 'https://www.naturalawakeningsmag.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/natural-awakenings-magazine/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.naturalawakeningsmag.com\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.naturalawakeningsmag.com\\' doesn\\'t match either of \\'shortener.secureserver.net\\', \\'www.shortener.secureserver.net\\'\")))'))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Revolution Radio (revolutionradio.org)',\n",
       "   'url': 'https://revolutionradio.org',\n",
       "   'report': 'https://mediabiasfactcheck.com/revolution-radio/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='revolutionradio.org', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1076)')))\"))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'Sheep Killers (www.sheepkillers.com)',\n",
       "   'url': 'https://www.sheepkillers.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/sheep-killers/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.ProtocolError('Connection aborted.',\n",
       "                                                                       OSError(0,\n",
       "                                                                               'Error')))),\n",
       " ({'category': 'conspiracy-pseudoscience',\n",
       "   'website_name': 'USA Hitman (usahitman.com)',\n",
       "   'url': 'https://usahitman.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/usa-hitman/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='usahitman.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)')))\"))),\n",
       " ({'category': 'pro-science',\n",
       "   'website_name': 'American Journal of Public Health (AJPH)',\n",
       "   'url': 'https://AJPH',\n",
       "   'report': 'https://mediabiasfactcheck.com/american-journal-of-public-health-ajph/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='ajph', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F6308B288>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"))),\n",
       " ({'category': 'pro-science',\n",
       "   'website_name': 'American Scientist (www.americanscientist.org)',\n",
       "   'url': 'https://www.americanscientist.org',\n",
       "   'report': 'https://mediabiasfactcheck.com/american-scientist/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.ProtocolError('Connection aborted.',\n",
       "                                                                       http.client.RemoteDisconnected('Remote end closed connection without response')))),\n",
       " ({'category': 'pro-science',\n",
       "   'website_name': 'Astronomy Magazine (www.astronomy.com)',\n",
       "   'url': 'https://www.astronomy.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/astronomy-magazine/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.astronomy.com\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.astronomy.com\\' doesn\\'t match either of \\'*.kalmbach.com\\', \\'kalmbach.com\\'\")))'))),\n",
       " ({'category': 'pro-science',\n",
       "   'website_name': 'Climate Science & Policy Watch (CSPW) (www.climatesciencewatch.org)',\n",
       "   'url': 'https://www.climatesciencewatch.org',\n",
       "   'report': 'https://mediabiasfactcheck.com/climate-science-and-policy-watch/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.climatesciencewatch.org\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.climatesciencewatch.org\\' doesn\\'t match either of \\'*.accountservergroup.com\\', \\'accountservergroup.com\\'\")))'))),\n",
       " ({'category': 'pro-science',\n",
       "   'website_name': 'National Academy of Sciences (NAS) (nationalacademyofsciences.org)',\n",
       "   'url': 'https://nationalacademyofsciences.org',\n",
       "   'report': 'https://mediabiasfactcheck.com/national-academy-of-sciences-nas/'},\n",
       "  requests.exceptions.ConnectionError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='nationalacademyofsciences.org', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000025F6387B048>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\"))),\n",
       " ({'category': 'pro-science',\n",
       "   'website_name': 'Next Observer (nextobserver.com)',\n",
       "   'url': 'https://nextobserver.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/next-observer/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='nextobserver.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)')))\"))),\n",
       " ({'category': 'pro-science',\n",
       "   'website_name': 'The Skeptics Dictionary (www.skepdic.com)',\n",
       "   'url': 'https://www.skepdic.com',\n",
       "   'report': 'https://mediabiasfactcheck.com/the-skeptics-dictionary/'},\n",
       "  requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError('HTTPSConnectionPool(host=\\'www.skepdic.com\\', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(\"hostname \\'www.skepdic.com\\' doesn\\'t match either of \\'*.secure.hostingprod.com\\', \\'secure.hostingprod.com\\'\")))')))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-minutes",
   "metadata": {},
   "source": [
    "Most of these are actually offline, as seen on manual inspection. The other ones will be discarded, as they are not numerous enough to impact any outcome in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-snake",
   "metadata": {},
   "source": [
    "#### Runs with Selenium\n",
    "\n",
    "*Note: A copy of the contents that have already been extracted will be created just in case...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "three-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_selenium = contents.copy()\n",
    "content_error_selenium = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "discrete-validity",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON URL # 947 out of 949 - https://www.who.int\n",
      "Progress:  99.79 %\n",
      "Current run time: 5.24 minutes\n",
      "Expected run time: 5.31 minutes\n"
     ]
    }
   ],
   "source": [
    "total_runs = 0\n",
    "start = timeit.default_timer()\n",
    "content_error = list()\n",
    "\n",
    "for entry in contents_selenium:\n",
    "    clear_output(wait=True)\n",
    "    total_runs += 1\n",
    "    \n",
    "    if 'number_facebook_urls' in entry:\n",
    "        if entry['number_facebook_urls'] > 0:\n",
    "            continue\n",
    "    else:\n",
    "        try:\n",
    "            if entry['url'] != None and entry['url'].startswith('http'):\n",
    "                soup = generate_selenium(entry['url'])\n",
    "                extracted_data = extract_facebook(soup)\n",
    "                entry['facebook_page'] = extracted_data[0]\n",
    "                entry['number_facebook_urls'] = extracted_data[1]\n",
    "\n",
    "            stop = timeit.default_timer()\n",
    "            expected_time = np.round((stop-start) / (total_runs / len(contents_selenium)) / 60, 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            content_error.append((entry, e))\n",
    "            continue\n",
    "\n",
    "    print('WORKING ON URL #', total_runs, 'out of', len(contents_selenium), '-', entry['url'])\n",
    "    print('Progress: ', np.round((total_runs/len(contents_selenium) * 100), 2), '%')\n",
    "    print('Current run time:', np.round((stop - start)/60, 2), 'minutes')\n",
    "    print('Expected run time:', expected_time, 'minutes') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-phoenix",
   "metadata": {},
   "source": [
    "### Creates DF and saves to Excel for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "designing-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(contents_selenium)\n",
    "df = df[df['number_facebook_urls'] > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "careful-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>website_name</th>\n",
       "      <th>url</th>\n",
       "      <th>report</th>\n",
       "      <th>facebook_page</th>\n",
       "      <th>number_facebook_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>least-biased</td>\n",
       "      <td>24ur.com</td>\n",
       "      <td>https://24ur.com</td>\n",
       "      <td>https://mediabiasfactcheck.com/24ur-com/</td>\n",
       "      <td>[https://facebook.com/24urcom]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>least-biased</td>\n",
       "      <td>38 North (www.38north.org)</td>\n",
       "      <td>https://www.38north.org</td>\n",
       "      <td>https://mediabiasfactcheck.com/38-north/</td>\n",
       "      <td>[https://www.facebook.com/38NorthNK]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>least-biased</td>\n",
       "      <td>680 News (www.680news.com)</td>\n",
       "      <td>https://www.680news.com</td>\n",
       "      <td>https://mediabiasfactcheck.com/680-news/</td>\n",
       "      <td>[https://www.facebook.com/680News]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>least-biased</td>\n",
       "      <td>1010 WINS AM (1010wins.radio.com)</td>\n",
       "      <td>https://1010wins.radio.com</td>\n",
       "      <td>https://mediabiasfactcheck.com/1010-wins-am/</td>\n",
       "      <td>[https://www.facebook.com/1010wins/, https://w...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>least-biased</td>\n",
       "      <td>ABC7Chicago.com</td>\n",
       "      <td>https://ABC7Chicago.com</td>\n",
       "      <td>https://mediabiasfactcheck.com/abc7chicago-com/</td>\n",
       "      <td>[https://www.facebook.com/pages/ABC-7-Chicago/...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>pro-science</td>\n",
       "      <td>Understanding Reality Through Science (underst...</td>\n",
       "      <td>https://understandrealitythroughscience.blogsp...</td>\n",
       "      <td>https://mediabiasfactcheck.com/understand-real...</td>\n",
       "      <td>[https://www.facebook.com/thom.raff]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>pro-science</td>\n",
       "      <td>VeryWell (www.verywell.com)</td>\n",
       "      <td>https://www.verywell.com</td>\n",
       "      <td>https://mediabiasfactcheck.com/verywell/</td>\n",
       "      <td>[https://www.facebook.com/verywell]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>pro-science</td>\n",
       "      <td>VeryWell Family (verywellfamily.com)</td>\n",
       "      <td>https://verywellfamily.com</td>\n",
       "      <td>https://mediabiasfactcheck.com/verywell-family/</td>\n",
       "      <td>[https://www.facebook.com/verywell]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>pro-science</td>\n",
       "      <td>World Meteorological Organization (public.wmo....</td>\n",
       "      <td>https://public.wmo.int</td>\n",
       "      <td>https://mediabiasfactcheck.com/world-meteorolo...</td>\n",
       "      <td>[https://www.facebook.com/World-Meteorological...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>pro-science</td>\n",
       "      <td>ZME Science (www.zmescience.com)</td>\n",
       "      <td>https://www.zmescience.com</td>\n",
       "      <td>https://mediabiasfactcheck.com/zme-science/</td>\n",
       "      <td>[https://www.facebook.com/zmescience]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                       website_name  \\\n",
       "0    least-biased                                           24ur.com   \n",
       "1    least-biased                         38 North (www.38north.org)   \n",
       "2    least-biased                         680 News (www.680news.com)   \n",
       "3    least-biased                  1010 WINS AM (1010wins.radio.com)   \n",
       "4    least-biased                                    ABC7Chicago.com   \n",
       "..            ...                                                ...   \n",
       "542   pro-science  Understanding Reality Through Science (underst...   \n",
       "543   pro-science                        VeryWell (www.verywell.com)   \n",
       "544   pro-science               VeryWell Family (verywellfamily.com)   \n",
       "545   pro-science  World Meteorological Organization (public.wmo....   \n",
       "546   pro-science                   ZME Science (www.zmescience.com)   \n",
       "\n",
       "                                                   url  \\\n",
       "0                                     https://24ur.com   \n",
       "1                              https://www.38north.org   \n",
       "2                              https://www.680news.com   \n",
       "3                           https://1010wins.radio.com   \n",
       "4                              https://ABC7Chicago.com   \n",
       "..                                                 ...   \n",
       "542  https://understandrealitythroughscience.blogsp...   \n",
       "543                           https://www.verywell.com   \n",
       "544                         https://verywellfamily.com   \n",
       "545                             https://public.wmo.int   \n",
       "546                         https://www.zmescience.com   \n",
       "\n",
       "                                                report  \\\n",
       "0             https://mediabiasfactcheck.com/24ur-com/   \n",
       "1             https://mediabiasfactcheck.com/38-north/   \n",
       "2             https://mediabiasfactcheck.com/680-news/   \n",
       "3         https://mediabiasfactcheck.com/1010-wins-am/   \n",
       "4      https://mediabiasfactcheck.com/abc7chicago-com/   \n",
       "..                                                 ...   \n",
       "542  https://mediabiasfactcheck.com/understand-real...   \n",
       "543           https://mediabiasfactcheck.com/verywell/   \n",
       "544    https://mediabiasfactcheck.com/verywell-family/   \n",
       "545  https://mediabiasfactcheck.com/world-meteorolo...   \n",
       "546        https://mediabiasfactcheck.com/zme-science/   \n",
       "\n",
       "                                         facebook_page  number_facebook_urls  \n",
       "0                       [https://facebook.com/24urcom]                   1.0  \n",
       "1                 [https://www.facebook.com/38NorthNK]                   1.0  \n",
       "2                   [https://www.facebook.com/680News]                   1.0  \n",
       "3    [https://www.facebook.com/1010wins/, https://w...                   3.0  \n",
       "4    [https://www.facebook.com/pages/ABC-7-Chicago/...                   1.0  \n",
       "..                                                 ...                   ...  \n",
       "542               [https://www.facebook.com/thom.raff]                   1.0  \n",
       "543                [https://www.facebook.com/verywell]                   1.0  \n",
       "544                [https://www.facebook.com/verywell]                   1.0  \n",
       "545  [https://www.facebook.com/World-Meteorological...                   1.0  \n",
       "546              [https://www.facebook.com/zmescience]                   1.0  \n",
       "\n",
       "[547 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continued-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_file = './data/interim/extracted_data.xlsx'\n",
    "df.to_excel(interim_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-creation",
   "metadata": {},
   "source": [
    "## Imports DF after cleaning and generates CrowdTangle's CSV file\n",
    "\n",
    "Generate a .csv using the `Page or Account URL,List` template.\n",
    "\n",
    "It will be saved to `./data/interim/crowdtangle_batch_upload.csv`. The `category` column will be the list name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dietary-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(interim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-chemical",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "needed-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['facebook_page'] = df['facebook_page'].apply(lambda x: x.replace(\"'\",\"\").replace('[','').\n",
    "                                                replace(']','').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "grave-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_export = df[['facebook_page','category']]\n",
    "df_to_export.columns = ['Page or Account URL','List']\n",
    "df_to_export.to_csv('./data/interim/crowdtangle_batch_upload.csv',\n",
    "                    sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-interference",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science-fakenews-env",
   "language": "python",
   "name": "science-fakenews-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
